{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ec0af5d",
      "metadata": {
        "id": "0ec0af5d"
      },
      "source": [
        "Logistic Regression — Binary & Multiclass (From Scratch) + EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc47fe1",
      "metadata": {
        "id": "4dc47fe1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer, load_iris, load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os\n",
        "plt.rcParams['figure.figsize'] = (6,4)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z-Q0F3AK5wTc",
      "metadata": {
        "id": "Z-Q0F3AK5wTc"
      },
      "source": [
        "\n",
        "## Math Primer (What You'll Implement)\n",
        "\n",
        "### 1) Binary Logistic Regression\n",
        "* We map $( z = \\mathbf{x}^\\top\\mathbf{w} + b $) through the sigmoid:\n",
        "$\n",
        "\\sigma(z) = \\frac{1}{1+e^{-z}}, \\qquad \\hat{y} = \\sigma(z) \\in (0,1).\n",
        "$\n",
        "<br>\n",
        "\n",
        "* The **binary cross-entropy** with L2 regularization ($(\\lambda$)) over \\(N\\) samples:\n",
        "$\n",
        "\\mathcal{L}(\\mathbf{w},b) = -\\frac{1}{N}\\sum_{i=1}^{N}\\Big[y_i\\log \\hat{y}_i + (1-y_i)\\log(1-\\hat{y}_i)\\Big] + \\frac{\\lambda}{2N}\\|\\mathbf{w}\\|_2^2.\n",
        "$\n",
        "\n",
        "<br><br>\n",
        "**Gradients**\n",
        "$\n",
        "\\nabla_{\\mathbf{w}} = \\tfrac{1}{N}\\mathbf{X}^\\top(\\hat{\\mathbf{y}}-\\mathbf{y}) + \\tfrac{\\lambda}{N}\\mathbf{w}, \\qquad\n",
        "\\nabla_b = \\tfrac{1}{N}\\sum_{i=1}^{N}(\\hat{y}_i-y_i).\n",
        "$\n",
        "\n",
        "We then update parameters with gradient descent $( \\mathbf{w}\\leftarrow\\mathbf{w}-\\eta\\nabla_{\\mathbf{w}},\\; b\\leftarrow b-\\eta \\nabla_b $).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a233e456",
      "metadata": {
        "id": "a233e456"
      },
      "source": [
        "---\n",
        "## Part A — Breast Cancer (Binary) — EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f33789",
      "metadata": {
        "id": "03f33789"
      },
      "source": [
        "**Goal:** Understand class balance, feature distributions, correlations, and a 2D PCA view before modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b87bd6",
      "metadata": {
        "id": "b2b87bd6"
      },
      "outputs": [],
      "source": [
        "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
        "feat_bc = load_breast_cancer().feature_names\n",
        "df_bc = pd.DataFrame(X_bc, columns=feat_bc)\n",
        "df_bc['target'] = y_bc\n",
        "print('Shape:', df_bc.shape)\n",
        "print('Class counts:')\n",
        "print(df_bc['target'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74cf30a5",
      "metadata": {
        "id": "74cf30a5"
      },
      "outputs": [],
      "source": [
        "plt.figure(); df_bc['target'].value_counts().sort_index().plot(kind='bar');\n",
        "plt.title('Breast Cancer — Class Counts'); plt.xlabel('Class (0=malignant, 1=benign)'); plt.ylabel('Count'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34dd245d",
      "metadata": {
        "id": "34dd245d"
      },
      "outputs": [],
      "source": [
        "for col in df_bc.columns[:10]:  # show first 10 features to avoid too many plots\n",
        "    plt.figure();\n",
        "    df_bc[df_bc['target']==0][col].plot(kind='hist', bins=30, alpha=0.5)\n",
        "    df_bc[df_bc['target']==1][col].plot(kind='hist', bins=30, alpha=0.5)\n",
        "    plt.title(f'Breast Cancer — Feature: {col}'); plt.xlabel(col); plt.ylabel('Frequency'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675b2f2e",
      "metadata": {
        "id": "675b2f2e"
      },
      "outputs": [],
      "source": [
        "corr = df_bc.drop(columns=['target']).corr()\n",
        "plt.figure(); plt.imshow(corr, aspect='auto'); plt.title('Breast Cancer — Correlation Heatmap'); plt.colorbar(); plt.xticks([]); plt.yticks([]); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8449cd",
      "metadata": {
        "id": "ee8449cd"
      },
      "outputs": [],
      "source": [
        "scaler_tmp = StandardScaler()\n",
        "Z = scaler_tmp.fit_transform(df_bc.drop(columns=['target']))\n",
        "Z2 = PCA(n_components=2).fit_transform(Z)\n",
        "plt.figure()\n",
        "for c in np.unique(y_bc):\n",
        "    pts = Z2[y_bc==c]\n",
        "    plt.scatter(pts[:,0], pts[:,1], s=16, label=f'class {c}')\n",
        "plt.legend(); plt.title('Breast Cancer — PCA (2D)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6291048a",
      "metadata": {
        "id": "6291048a"
      },
      "source": [
        "### Modeling Setup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be619e3",
      "metadata": {
        "id": "8be619e3"
      },
      "outputs": [],
      "source": [
        "Xtr_bc, Xva_bc, ytr_bc, yva_bc = train_test_split(X_bc, y_bc, test_size=0.25, stratify=y_bc, random_state=42)\n",
        "scaler_bc = StandardScaler()\n",
        "Xtr_bc_std = scaler_bc.fit_transform(Xtr_bc)\n",
        "Xva_bc_std = scaler_bc.transform(Xva_bc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a245350",
      "metadata": {
        "id": "2a245350"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    # ------------------------------------------------\n",
        "    # TODO Refer to the Binary Logistic Regression Sigmoid Function defined in the Math Primer (Exercise 1)\n",
        "    # You may want to use np.exp\n",
        "    # ------------------------------------------------\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed0da7e",
      "metadata": {
        "id": "1ed0da7e"
      },
      "outputs": [],
      "source": [
        "def bin_loss_and_grads(X, y, w, b, lam):\n",
        "    N = X.shape[0]\n",
        "    z = X @ w + b\n",
        "    p = sigmoid(z)\n",
        "    eps = 1e-12\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # TODO Refer to the Binary Cross-Entropy Loss Function defined in the Math Primer (Exercise 2)\n",
        "    # You may want to use np.exp\n",
        "    # ------------------------------------------------\n",
        "    ce = -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n",
        "    reg = (lam/(2.0*N))*(w@w)\n",
        "    loss = ce + reg\n",
        "    gw = (X.T @ (p - y))/N + (lam/N)*w\n",
        "    gb = np.mean(p - y)\n",
        "    return loss, gw, gb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49f687d",
      "metadata": {
        "id": "e49f687d"
      },
      "outputs": [],
      "source": [
        "def fit_logreg_binary(X, y, lr=0.1, lam=0.01, epochs=2000, batch_size=None, verbose=True):\n",
        "    N, D = X.shape; w = np.zeros(D); b = 0.0; losses = []\n",
        "    for t in range(1, epochs+1):\n",
        "        if batch_size is None:\n",
        "            Xb, yb = X, y\n",
        "        else:\n",
        "            idx = np.random.choice(N, size=batch_size, replace=False); Xb, yb = X[idx], y[idx]\n",
        "        loss, gw, gb = bin_loss_and_grads(Xb, yb, w, b, lam)\n",
        "        w -= lr*gw; b -= lr*gb\n",
        "        L,_,_ = bin_loss_and_grads(X, y, w, b, lam); losses.append(L)\n",
        "        if verbose and (t % max(1, epochs//10) == 0):\n",
        "            pass\n",
        "    return w, b, np.array(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "303f2a60",
      "metadata": {
        "id": "303f2a60"
      },
      "outputs": [],
      "source": [
        "w_bc, b_bc, losses_bc = fit_logreg_binary(Xtr_bc_std, ytr_bc)\n",
        "if len(losses_bc)>0:\n",
        "    plt.figure(); plt.plot(losses_bc); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Breast Cancer: Training Loss'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b9d7c8",
      "metadata": {
        "id": "b9b9d7c8"
      },
      "outputs": [],
      "source": [
        "def predict_proba_binary(X, w, b):\n",
        "    # ------------------------------------------------\n",
        "    # TODO Refer to the Binary Logistic Regression Sigmoid Function defined in the Math Primer (Exercise 3)\n",
        "    # What is the Z defined as\n",
        "    # ------------------------------------------------\n",
        "    return sigmoid(X @ w + b)\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511437e8",
      "metadata": {
        "id": "511437e8"
      },
      "outputs": [],
      "source": [
        "def predict_label_binary(X, w, b, thresh=0.5):\n",
        "    # ------------------------------------------------\n",
        "    # TODO We want to determine the binary label of the data if it is above a certain threshold (Exercise 4)\n",
        "    # You may want to return it as a type int\n",
        "    # ------------------------------------------------\n",
        "    return (predict_proba_binary(X, w, b) >= thresh).astype(int)\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62be4195",
      "metadata": {
        "id": "62be4195"
      },
      "outputs": [],
      "source": [
        "p_bc = predict_proba_binary(Xva_bc_std, w_bc, b_bc)\n",
        "yhat_bc = predict_label_binary(Xva_bc_std, w_bc, b_bc)\n",
        "print('Breast Cancer — Acc/Prec/Rec/F1/AUC:',\n",
        "      round(accuracy_score(yva_bc, yhat_bc),4),\n",
        "      round(precision_score(yva_bc, yhat_bc),4),\n",
        "      round(recall_score(yva_bc, yhat_bc),4),\n",
        "      round(f1_score(yva_bc, yhat_bc),4),\n",
        "      round(roc_auc_score(yva_bc, p_bc),4))\n",
        "fpr, tpr, _ = roc_curve(yva_bc, p_bc)\n",
        "plt.figure(); plt.plot(fpr, tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC — Breast Cancer'); plt.show()\n",
        "cm = confusion_matrix(yva_bc, yhat_bc)\n",
        "print(cm)\n",
        "plt.figure(); plt.imshow(cm, aspect='auto'); plt.title('Confusion Matrix — Breast Cancer'); plt.xlabel('Pred'); plt.ylabel('True'); plt.colorbar(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UFKGMxRLyiHY",
      "metadata": {
        "id": "UFKGMxRLyiHY"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------\n",
        "# TODO We will be utilizing the LogisticRegression model from sklearn, we will use a max_iter=10000 (Exercise 5)\n",
        "# First we will train on our data Xtr_bc_std\n",
        "# After we want to do predictions on the validation set data Xva_bc_std\n",
        "# ------------------------------------------------\n",
        "clf_bc = LogisticRegression(max_iter=10000)\n",
        "clf_bc.fit(Xtr_bc_std, ytr_bc)\n",
        "pred_bc = clf_bc.predict(Xva_bc_std)\n",
        "\n",
        "# ----------------------------\n",
        "# Implementation Ends Here\n",
        "# ----------------------------\n",
        "\n",
        "prob_bc = clf_bc.predict_proba(Xva_bc_std)[:,1]\n",
        "print('sklearn — Acc/Prec/Rec/F1/AUC:',\n",
        "      round(accuracy_score(yva_bc, pred_bc),4),\n",
        "      round(precision_score(yva_bc, pred_bc),4),\n",
        "      round(recall_score(yva_bc, pred_bc),4),\n",
        "      round(f1_score(yva_bc, pred_bc),4),\n",
        "      round(roc_auc_score(yva_bc, prob_bc),4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6de2cf0",
      "metadata": {
        "id": "f6de2cf0"
      },
      "source": [
        "---\n",
        "## Part B — Titanic (Binary) — EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a92cb5d",
      "metadata": {
        "id": "7a92cb5d"
      },
      "source": [
        "**Goal:** Inspect missingness and the relationship between survival and key variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb04fd1d",
      "metadata": {
        "id": "bb04fd1d"
      },
      "outputs": [],
      "source": [
        "TRAIN_CSV = 'train.csv'\n",
        "TEST_CSV  = 'test.csv'\n",
        "if not os.path.exists(TRAIN_CSV):\n",
        "    print('Place Kaggle Titanic train.csv beside the notebook to run this section.')\n",
        "else:\n",
        "    titanic = pd.read_csv(TRAIN_CSV)\n",
        "    print('Shape:', titanic.shape)\n",
        "    print(titanic.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2154a049",
      "metadata": {
        "id": "2154a049"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(TRAIN_CSV):\n",
        "    plt.figure(); titanic['Survived'].value_counts().sort_index().plot(kind='bar');\n",
        "    plt.title('Titanic — Class Counts'); plt.xlabel('Survived'); plt.ylabel('Count'); plt.show()\n",
        "    plt.figure(); titanic['Age'].dropna().plot(kind='hist', bins=30); plt.title('Age Distribution'); plt.xlabel('Age'); plt.show()\n",
        "    plt.figure(); titanic['Fare'].dropna().plot(kind='hist', bins=30); plt.title('Fare Distribution'); plt.xlabel('Fare'); plt.show()\n",
        "    plt.figure(); titanic.groupby('Sex')['Survived'].mean().plot(kind='bar'); plt.title('Survival Rate by Sex'); plt.ylabel('Rate'); plt.show()\n",
        "    plt.figure(); titanic.groupby('Pclass')['Survived'].mean().plot(kind='bar'); plt.title('Survival Rate by Pclass'); plt.ylabel('Rate'); plt.show()\n",
        "    if titanic['Embarked'].notna().any():\n",
        "        plt.figure(); titanic.groupby('Embarked')['Survived'].mean().plot(kind='bar'); plt.title('Survival Rate by Embarked'); plt.ylabel('Rate'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f470a6f",
      "metadata": {
        "id": "6f470a6f"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(TRAIN_CSV):\n",
        "    # Basic numeric PCA view\n",
        "    num_cols = ['Pclass','Age','SibSp','Parch','Fare']\n",
        "    tmp = titanic[num_cols].copy()\n",
        "    tmp = tmp.fillna(tmp.median())\n",
        "    Z = StandardScaler().fit_transform(tmp)\n",
        "    Z2 = PCA(n_components=2).fit_transform(Z)\n",
        "    y = titanic['Survived'].values\n",
        "    plt.figure()\n",
        "    for c in np.unique(y):\n",
        "        pts = Z2[y==c]\n",
        "        plt.scatter(pts[:,0], pts[:,1], s=14, label=f'Survived={c}')\n",
        "    plt.legend(); plt.title('Titanic — PCA (numeric features)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d92a1c5",
      "metadata": {
        "id": "8d92a1c5"
      },
      "source": [
        "### Titanic — Modeling (reuse binary functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2be2187",
      "metadata": {
        "id": "a2be2187"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(TRAIN_CSV):\n",
        "    df = titanic.copy()\n",
        "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
        "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "    df['Sex'] = df['Sex'].map({'male':0,'female':1})\n",
        "    df = pd.get_dummies(df, columns=['Embarked'], drop_first=True)\n",
        "    features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked_Q','Embarked_S']\n",
        "    X_ti = df[features]\n",
        "    y_ti = df['Survived']\n",
        "    Xtr_ti, Xva_ti, ytr_ti, yva_ti = train_test_split(X_ti, y_ti, test_size=0.25, stratify=y_ti, random_state=42)\n",
        "    scaler_ti = StandardScaler()\n",
        "    Xtr_ti_std = scaler_ti.fit_transform(Xtr_ti)\n",
        "    Xva_ti_std = scaler_ti.transform(Xva_ti)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # TODO We will utilize the fit_logreg_binary we defined in the first part, we will be using the Xtr_ti_std as the train data (Exercise 6)\n",
        "    # lr=0.1, lam=0.01, epochs=1500 and verbose=False\n",
        "    # ------------------------------------------------\n",
        "    w_ti, b_ti, losses_ti = fit_logreg_binary(Xtr_ti_std, ytr_ti.to_numpy(), lr=0.1, lam=0.01, epochs=1500, verbose=False)\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n",
        "\n",
        "    plt.figure(); plt.plot(losses_ti); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Titanic: Training Loss'); plt.show()\n",
        "    p_ti = predict_proba_binary(Xva_ti_std, w_ti, b_ti)\n",
        "    yhat_ti = predict_label_binary(Xva_ti_std, w_ti, b_ti)\n",
        "    print('Titanic Acc/Prec/Rec/F1/AUC:', round(accuracy_score(yva_ti, yhat_ti),4), round(precision_score(yva_ti, yhat_ti),4), round(recall_score(yva_ti, yhat_ti),4), round(f1_score(yva_ti, yhat_ti),4), round(roc_auc_score(yva_ti, p_ti),4))\n",
        "\n",
        "    clf_ti = LogisticRegression(max_iter=10000)\n",
        "    clf_ti.fit(Xtr_ti_std, ytr_ti)\n",
        "    pred_ti = clf_ti.predict(Xva_ti_std)\n",
        "    prob_ti = clf_ti.predict_proba(Xva_ti_std)[:,1]\n",
        "    print('sklearn (Titanic):', round(accuracy_score(yva_ti, pred_ti),4), round(precision_score(yva_ti, pred_ti),4), round(recall_score(yva_ti, pred_ti),4), round(f1_score(yva_ti, pred_ti),4), round(roc_auc_score(yva_ti, prob_ti),4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb246c2b",
      "metadata": {
        "id": "fb246c2b"
      },
      "source": [
        "---\n",
        "## Part C — Iris (Multiclass) — EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3gxHaIM-6es4",
      "metadata": {
        "id": "3gxHaIM-6es4"
      },
      "source": [
        "### 2) Multiclass Logistic Regression (Softmax)\n",
        "For \\(K\\) classes, with $(\\mathbf{W}\\in\\mathbb{R}^{D\\times K}$), $(\\mathbf{b}\\in\\mathbb{R}^{K}$):\n",
        "$\n",
        "\\mathbf{z} = \\mathbf{W}^\\top \\mathbf{x} + \\mathbf{b},\\qquad\n",
        "\\hat{p}_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}} = \\text{softmax}(\\mathbf{z})_k.\n",
        "$\n",
        "\n",
        "<br><br>\n",
        "Use one-hot labels $( \\mathbf{Y}\\in\\{0,1\\}^{N\\times K} $) and predictions $( \\hat{\\mathbf{P}}\\in[0,1]^{N\\times K} $):\n",
        "\n",
        "**Total Loss Function**: $\n",
        "\\mathcal{L}(\\mathbf{W},\\mathbf{b}) = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K Y_{ik}\\log \\hat{P}_{ik} + \\frac{\\lambda}{2N}\\|\\mathbf{W}\\|_F^2.\n",
        "$\n",
        "\n",
        "<br><br>\n",
        "**Gradients:**\n",
        "$\n",
        "\\nabla_{\\mathbf{W}} = \\tfrac{1}{N}\\mathbf{X}^\\top(\\hat{\\mathbf{P}}-\\mathbf{Y}) + \\tfrac{\\lambda}{N}\\mathbf{W}, \\qquad\n",
        "\\nabla_{\\mathbf{b}} = \\tfrac{1}{N}\\sum_{i=1}^{N}(\\hat{\\mathbf{P}}_{i\\cdot}-\\mathbf{Y}_{i\\cdot}).\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2091a1a0",
      "metadata": {
        "id": "2091a1a0"
      },
      "source": [
        "**Goal:** Visualize distributions and separability across the 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d72e9e",
      "metadata": {
        "id": "51d72e9e"
      },
      "outputs": [],
      "source": [
        "X_ir, y_ir = load_iris(return_X_y=True)\n",
        "feat_ir = load_iris().feature_names\n",
        "df_ir = pd.DataFrame(X_ir, columns=feat_ir)\n",
        "df_ir['target'] = y_ir\n",
        "print('Class counts:', df_ir['target'].value_counts().sort_index().to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46957daa",
      "metadata": {
        "id": "46957daa"
      },
      "outputs": [],
      "source": [
        "for col in feat_ir:\n",
        "    plt.figure();\n",
        "    for c in np.unique(y_ir):\n",
        "        df_ir[df_ir['target']==c][col].plot(kind='hist', bins=25, alpha=0.5)\n",
        "    plt.title(f'Iris — Feature: {col}'); plt.xlabel(col); plt.ylabel('Frequency'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1daa66f7",
      "metadata": {
        "id": "1daa66f7"
      },
      "outputs": [],
      "source": [
        "scaler_tmp = StandardScaler()\n",
        "Z = scaler_tmp.fit_transform(df_ir.drop(columns=['target']))\n",
        "Z2 = PCA(n_components=2).fit_transform(Z)\n",
        "plt.figure()\n",
        "for c in np.unique(y_ir):\n",
        "    pts = Z2[y_ir==c]\n",
        "    plt.scatter(pts[:,0], pts[:,1], s=16, label=f'class {c}')\n",
        "plt.legend(); plt.title('Iris — PCA (2D)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e50f96ff",
      "metadata": {
        "id": "e50f96ff"
      },
      "source": [
        "### Iris — Modeling (Softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e4cbdd",
      "metadata": {
        "id": "34e4cbdd"
      },
      "outputs": [],
      "source": [
        "Xtr_ir, Xva_ir, ytr_ir, yva_ir = train_test_split(X_ir, y_ir, test_size=0.25, stratify=y_ir, random_state=42)\n",
        "scaler_ir = StandardScaler()\n",
        "Xtr_ir_std = scaler_ir.fit_transform(Xtr_ir)\n",
        "Xva_ir_std = scaler_ir.transform(Xva_ir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1114c5",
      "metadata": {
        "id": "6c1114c5"
      },
      "outputs": [],
      "source": [
        "def one_hot(y, K):\n",
        "    Y = np.zeros((y.shape[0], K)); Y[np.arange(y.shape[0]), y] = 1.0; return Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3f1a0e",
      "metadata": {
        "id": "3d3f1a0e"
      },
      "outputs": [],
      "source": [
        "def softmax(Z):\n",
        "    Zs = Z - Z.max(axis=1, keepdims=True)\n",
        "    # ------------------------------------------------\n",
        "    # TODO Refer to the Multiclass Logistic Regression Softmax section above (Exercise 7)\n",
        "    # You may want to use np.exp\n",
        "    # ------------------------------------------------\n",
        "    expZ = np.exp(Zs)\n",
        "    return expZ / expZ.sum(axis=1, keepdims=True)\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb952ea",
      "metadata": {
        "id": "ddb952ea"
      },
      "outputs": [],
      "source": [
        "def softmax_loss_and_grads(X, Y, W, b, lam):\n",
        "    N = X.shape[0]\n",
        "    Z = X @ W + b\n",
        "    P = softmax(Z)\n",
        "    eps = 1e-12\n",
        "    # ------------------------------------------------\n",
        "    # TODO Refer to the Multiclass Logistic Regression Total Loss function defined above (Exercise 8)\n",
        "    # You may want to use np.log\n",
        "    # ------------------------------------------------\n",
        "    ce = -np.sum(Y * np.log(P + eps)) / N\n",
        "    reg = (lam / (2.0 * N)) * np.sum(W * W)\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n",
        "\n",
        "    loss = ce + reg\n",
        "    dZ = (P - Y)/N\n",
        "    dW = X.T @ dZ + (lam/N)*W\n",
        "    db = dZ.sum(axis=0)\n",
        "    return loss, dW, db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44edad2d",
      "metadata": {
        "id": "44edad2d"
      },
      "outputs": [],
      "source": [
        "def fit_softmax(X, y, lr=0.1, lam=0.01, epochs=1500, batch_size=None, verbose=True):\n",
        "    N, D = X.shape; K=int(y.max()+1); Y=one_hot(y,K); W=np.zeros((D,K)); b=np.zeros(K); losses=[]\n",
        "    for t in range(1, epochs+1):\n",
        "        if batch_size is None:\n",
        "            Xb, Yb = X, Y\n",
        "        else:\n",
        "            idx = np.random.choice(N, size=batch_size, replace=False); Xb, Yb = X[idx], Y[idx]\n",
        "        loss, dW, db = softmax_loss_and_grads(Xb, Yb, W, b, lam)\n",
        "        W -= lr*dW; b -= lr*db\n",
        "        L,_,_ = softmax_loss_and_grads(X, Y, W, b, lam); losses.append(L)\n",
        "        if verbose and (t % max(1, epochs//10)==0):\n",
        "            pass\n",
        "    return W, b, np.array(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612e4f7a",
      "metadata": {
        "id": "612e4f7a"
      },
      "outputs": [],
      "source": [
        "W_ir, b_ir, losses_ir = fit_softmax(Xtr_ir_std, ytr_ir, lr=0.1, lam=0.01, epochs=1500, verbose=True)\n",
        "if len(losses_ir)>0:\n",
        "    plt.figure(); plt.plot(losses_ir); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Iris: Training Loss'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e249c3e",
      "metadata": {
        "id": "6e249c3e"
      },
      "outputs": [],
      "source": [
        "def predict_proba_softmax(X, W, b):\n",
        "    # TODO Refer to the Multiclass Logistic Regression Softmax section above (Exercise 9)\n",
        "    # Hint what is Z defined as\n",
        "   return softmax(X @ W + b)\n",
        "    # ----------------------------\n",
        "    # Implementation Ends Here\n",
        "    # ----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9256743",
      "metadata": {
        "id": "b9256743"
      },
      "outputs": [],
      "source": [
        "def predict_label_softmax(X, W, b):\n",
        "    return np.argmax(predict_proba_softmax(X, W, b), axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f4b783",
      "metadata": {
        "id": "40f4b783"
      },
      "outputs": [],
      "source": [
        "yp_ir = predict_label_softmax(Xva_ir_std, W_ir, b_ir)\n",
        "print('Iris accuracy:', round(accuracy_score(yva_ir, yp_ir),4))\n",
        "cm = confusion_matrix(yva_ir, yp_ir); print(cm)\n",
        "plt.figure(); plt.imshow(cm, aspect='auto'); plt.title('Confusion Matrix — Iris'); plt.xlabel('Pred'); plt.ylabel('True'); plt.colorbar(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3VYCBY_y-KZ",
      "metadata": {
        "id": "f3VYCBY_y-KZ"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------\n",
        "# TODO We will be utilizing the LogisticRegression model from sklearn, we will use multinomial as the multi_class, lbfgs as the solver and the max_iter=2000 (Exercise 10)\n",
        "# First we will train on our data Xtr_ir_std\n",
        "# After we want to do predictions on the validation set data Xva_ir_std\n",
        "# ------------------------------------------------\n",
        "clf_ir = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000)\n",
        "clf_ir.fit(Xtr_ir_std, ytr_ir)\n",
        "pred_ir = clf_ir.predict(Xva_ir_std)\n",
        "# ----------------------------\n",
        "# Implementation Ends Here\n",
        "# ----------------------------\n",
        "\n",
        "print('sklearn (Iris) accuracy:', round(accuracy_score(yva_ir, pred_ir),4))\n",
        "print(classification_report(yva_ir, pred_ir, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b44b28",
      "metadata": {
        "id": "54b44b28"
      },
      "source": [
        "---\n",
        "## Part D — Digits (Multiclass) — EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f78151",
      "metadata": {
        "id": "d1f78151"
      },
      "source": [
        "**Goal:** See sample digits, per-class averages, and separability in PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d849da",
      "metadata": {
        "id": "d2d849da"
      },
      "outputs": [],
      "source": [
        "X_dig, y_dig = load_digits(return_X_y=True)\n",
        "print('Digits shape:', X_dig.shape)\n",
        "imgs = X_dig.reshape(-1, 8, 8)\n",
        "for i in range(12):\n",
        "    plt.figure(); plt.imshow(imgs[i], cmap=None); plt.title(f'Sample digit: {y_dig[i]}'); plt.axis('off'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dade2df4",
      "metadata": {
        "id": "dade2df4"
      },
      "outputs": [],
      "source": [
        "avg_imgs = []\n",
        "for k in np.unique(y_dig):\n",
        "    avg_imgs.append(imgs[y_dig==k].mean(axis=0))\n",
        "avg_imgs = np.array(avg_imgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ada6210",
      "metadata": {
        "id": "7ada6210"
      },
      "outputs": [],
      "source": [
        "for k in range(avg_imgs.shape[0]):\n",
        "    plt.figure(); plt.imshow(avg_imgs[k], cmap=None); plt.title(f'Average image for class {k}'); plt.axis('off'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e578e0bd",
      "metadata": {
        "id": "e578e0bd"
      },
      "outputs": [],
      "source": [
        "Z = StandardScaler().fit_transform(X_dig)\n",
        "Z2 = PCA(n_components=2).fit_transform(Z)\n",
        "plt.figure()\n",
        "for c in np.unique(y_dig):\n",
        "    pts = Z2[y_dig==c]\n",
        "    plt.scatter(pts[:,0], pts[:,1], s=12, label=str(c))\n",
        "plt.legend(); plt.title('Digits — PCA (2D)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d6a3f9",
      "metadata": {
        "id": "a0d6a3f9"
      },
      "source": [
        "### Digits — Modeling (Softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216d83b5",
      "metadata": {
        "id": "216d83b5"
      },
      "outputs": [],
      "source": [
        "Xtr_d, Xva_d, ytr_d, yva_d = train_test_split(X_dig, y_dig, test_size=0.25, stratify=y_dig, random_state=42)\n",
        "scaler_d = StandardScaler()\n",
        "Xtr_d_std = scaler_d.fit_transform(Xtr_d)\n",
        "Xva_d_std = scaler_d.transform(Xva_d)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# TODO We will utilize the fit_softmax we defined in the first part, we will be using the Xtr_d_std as the train data (Exercise 11)\n",
        "# lr=0.2, lam=0.001, epochs=1500 and verbose=False\n",
        "# ------------------------------------------------\n",
        "W_d, b_d, losses_d = fit_softmax(Xtr_d_std, ytr_d, lr=0.2, lam=0.001, epochs=1500, verbose=False)\n",
        "# ----------------------------\n",
        "# Implementation Ends Here\n",
        "# ----------------------------\n",
        "\n",
        "if len(losses_d)>0:\n",
        "    plt.figure(); plt.plot(losses_d); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Digits: Training Loss'); plt.show()\n",
        "yp_d = predict_label_softmax(Xva_d_std, W_d, b_d)\n",
        "print('Digits accuracy (from scratch):', round(accuracy_score(yva_d, yp_d),4))\n",
        "cm = confusion_matrix(yva_d, yp_d); print(cm)\n",
        "plt.figure(); plt.imshow(cm, aspect='auto'); plt.title('Confusion Matrix — Digits'); plt.xlabel('Pred'); plt.ylabel('True'); plt.colorbar(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oArd_LzjzG06",
      "metadata": {
        "id": "oArd_LzjzG06"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------------------\n",
        "# TODO We will be utilizing the LogisticRegression model from sklearn, we will use multinomial as the multi_class, lbfgs as the solver and the max_iter=2000 (Exercise 12)\n",
        "# First we will train on our data Xtr_d_std\n",
        "# After we want to do predictions on the validation set data Xva_d_std\n",
        "# ------------------------------------------------\n",
        "\n",
        "clf_d = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=2000)\n",
        "clf_d.fit(Xtr_d_std, ytr_d)\n",
        "preds_skl_d = clf_d.predict(Xva_d_std)\n",
        "# ----------------------------\n",
        "# Implementation Ends Here\n",
        "# ----------------------------\n",
        "\n",
        "print('sklearn (Digits) accuracy:', round(accuracy_score(yva_d, preds_skl_d),4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyn4S0wK7Xje",
      "metadata": {
        "id": "tyn4S0wK7Xje"
      },
      "source": [
        "### **1. Sigmoid vs Softmax**  \n",
        "\n",
        "Why do we use the sigmoid or softmax functions instead of a linear output?\n",
        "\n",
        "**Answer:** Linear outputs are unbounded which makes them unsuitable for representing probabilities.\n",
        "\n",
        "\n",
        "### **2. Separating Data** \n",
        "\n",
        "Why can Logistic Regression handle linearly separable data well, but struggle with non-linear data?\n",
        "\n",
        "**Answer:** It performs well on linearly separable data because it learns only a linear decision boundary, causing it to struggle when the true boundary is non-linear.\n",
        "\n",
        "\n",
        "### **3. Logistic vs Linear** \n",
        "\n",
        "Compare Logistic Regression with Linear Regression. In what cases can both models produce similar predictions?\n",
        "\n",
        "**Answer:** They can produce similar predictions when the data is nearly linearly separable and when the outputs falls within the 0–1 range.\n",
        "\n",
        "### **4. Discriminative Model** \n",
        "\n",
        "Explain why Logistic Regression is a discriminative model and not a generative one.\n",
        "\n",
        "**Answer:** It's discriminative because it directly models P(y∣x) rather than modeling how the data x is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K0bNKELR7YFK",
      "metadata": {
        "id": "K0bNKELR7YFK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
